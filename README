# Terraform AWS Data Engineering Project Setup Guide

## Project Structure Overview

```
terraform-data-project/
├── main.tf                 # Root configuration that calls modules
├── variables.tf           # Root-level input variables
├── outputs.tf            # Root-level outputs
├── terraform.tfvars      # Variable values (your customizations)
└── modules/
    └── rds/
        ├── main.tf        # RDS module implementation
        ├── variables.tf   # Module input variables
        └── outputs.tf     # Module outputs
```

## Key Concepts for Data Engineering

### Why This Structure Matters

**Modularity**: The RDS module can be reused across different environments (dev, staging, prod) or projects. As a former full-stack developer, think of modules like reusable components in React.

**Separation of Concerns**:

- **Root configuration**: Orchestrates the overall infrastructure
- **Module**: Contains the specific logic for creating RDS resources
- **Variables**: Make everything configurable without hardcoding values

### RDS Configuration for Data Engineering

**Parameter Group Settings**:

- `binlog_format = ROW`: Essential for ETL processes - captures row-level changes
- `binlog_row_image = full`: Records complete row data, crucial for CDC (Change Data Capture)

**Why These Parameters Matter**:

- **ETL Processes**: Row-based binary logging allows you to track individual record changes
- **Data Pipeline Integration**: Tools like Debezium, AWS DMS can read these logs for real-time data streaming
- **Audit Trails**: Full row images help with data lineage and debugging

## Prerequisites

1. **AWS CLI configured** with appropriate credentials
2. **Terraform installed** (version 1.0+)
3. **AWS Account** with permissions to create RDS resources

## Step-by-Step Deployment

### 1. Initialize Terraform

```bash
# Navigate to your project directory
cd terraform-data-project

# Initialize Terraform (downloads providers and modules)
terraform init
```

### 2. Plan the Deployment

```bash
# See what resources will be created
terraform plan
```

### 3. Apply the Configuration

```bash
# Create the resources
terraform apply

# Type 'yes' when prompted
```

### 4. Connect to Your Database

After successful deployment, use the outputs to connect:

```bash
# Get the database endpoint
terraform output rds_endpoint

# Example connection with MySQL client
mysql -h [endpoint] -u admin -p naya_de_db
```

## Important Notes for Learning

### Security Considerations

- **Password Management**: In production, use AWS Secrets Manager instead of plain text passwords
- **Network Access**: The `publicly_accessible = true` setting is for learning only
- **VPC Configuration**: Production deployments should use private subnets

### Cost Management

- **Free Tier**: `db.t3.micro` is free tier eligible for 12 months
- **Storage**: Auto-scaling is enabled (20GB minimum, 100GB maximum)
- **Backups**: 7-day retention balances learning needs with cost

### Cleanup

```bash
# When you're done learning, clean up resources to avoid charges
terraform destroy
```

## Next Steps for ETL Learning

1. **Connect Python**: Use SQLAlchemy or pymysql to connect from Python
2. **Load Sample Data**: Practice INSERT operations and data loading
3. **Monitor Binary Logs**: Learn to read MySQL binary logs for CDC
4. **Add S3 Integration**: Next module could be S3 for data lake functionality

## Troubleshooting Common Issues

### AWS Credentials

```bash
# Check if AWS CLI is configured
aws sts get-caller-identity
```

### Terraform State

```bash
# If you encounter state issues
terraform refresh

# To see current state
terraform show
```

### Database Connection

- Ensure your IP is allowed in the security group (Terraform uses default VPC security group)
- Wait 5-10 minutes after `terraform apply` for the database to be fully available

### After apply

parameter_group_name = "naya-de-proj-db-pg"
rds_endpoint = "naya-de-proj-mysql-db.cdwsmiaocq0o.us-east-1.rds.amazonaws.com:3306"
rds_identifier = "naya-de-proj-mysql-db"
rds_port = 3306

local workbench
username hrylog
password: harry8mysql

rds details :
VPC security groups
default (sg-03625d41b582de0e1)

rds endpoint naya-de-proj-mysql-db.cdwsmiaocq0o.us-east-1.rds.amazonaws.com
port 3306
username admin
password hrylog8DVA02
rds pwd naya-de-proj-pwd-mysql8

### Destroy just Redshift module (saves ~$53/day)

terraform destroy -target=module.redshift_warehouse

### Destroy VPC endpoints too (if not needed for other services)  

terraform destroy -target=module.glue_etl.aws_vpc_endpoint.redshift
terraform destroy -target=module.glue_etl.aws_vpc_endpoint.secretsmanager
terraform destroy -target=module.glue_etl.aws_vpc_endpoint.sts
terraform destroy -target=module.glue_etl.aws_vpc_endpoint.s3_gateway

### Keep: RDS (~$0.40/day), DMS (~$0.40/day), S3/Lambda (minimal)

### Total cost drops from ~$55/day to ~$2/day

### Destroy only expensive components

terraform destroy -target=module.redshift_warehouse

### Keep everything else running ($2-3/day)

### - RDS, DMS, S3, Lambda, Glue jobs (metadata only)

### - All your development work preserved

### Next day - recreate Redshift with full automation

terraform apply -target=module.redshift_warehouse

### Tables and procedures created automatically via null_resource


CREATE DATABASE production;
CREATE SCHEMA sales;
SET search_path TO sales;

Table creation scripts (you have these files):


customer_setup_sql.sql
products_setup_sql.sql
orders_setup_sql.sql
order_details_setup_sql.sql


Stored procedures (you have these files):


customer_dim_sp.sql
products_dim_sp.sql



Bronze S3 (raw data) → Glue ETL jobs (transform) → Silver S3 (cleaned) → Glue load jobs → Redshift tables (final)


Project Summary: End-to-End Data Engineering Pipeline
This is a comprehensive modern data lake and warehouse architecture built on AWS using Infrastructure as Code (Terraform). Here's the complete flow:
Architecture Overview
MySQL (Source) → DMS → S3 Data Lake → Glue ETL → Redshift → Step Functions Orchestration
Components Built

MySQL RDS - Source transactional database with sample retail data
DMS (Database Migration Service) - Captures changes from MySQL to S3
S3 Data Lake - Bronze (raw) and Silver (processed) data layers
AWS Glue - ETL jobs for data transformation and loading
Redshift Serverless - Data warehouse with dimensional modeling
Step Functions - Orchestrates the entire ETL pipeline
Supporting Services - IAM, Secrets Manager, VPC endpoints, Lambda

Data Processing Flow

Ingestion: DMS streams MySQL data to S3 bronze layer
Transformation: Glue jobs clean and transform bronze → silver data
Loading: Glue jobs load transformed data into Redshift dimension/fact tables
Orchestration: Step Functions coordinates parallel execution with error handling

Key Technical Challenges Solved

Networking: VPC endpoints for private service communication
Security: Secrets management, IAM policies, role-based access
Scalability: Parallel processing, serverless compute
Error Handling: Retry logic, monitoring, failure recovery
Cost Management: Daily destroy/recreate patterns for development

Required Data Knowledge
Essential Skills

SQL - Database querying, joins, dimensional modeling concepts
Data Modeling - Understanding fact tables, dimension tables, star schema
ETL Concepts - Extract, Transform, Load patterns and best practices
Cloud Architecture - AWS services interaction and data flow patterns

Terraform/Infrastructure

Infrastructure as Code principles
Module-based architecture
Variable management and dependencies

AWS Services Understanding

S3: Data lake storage patterns (bronze/silver/gold)
Glue: Spark-based ETL jobs and Data Catalog
Redshift: Columnar data warehouse, query optimization
Step Functions: State machines, parallel execution, error handling
DMS: Change data capture, replication concepts

Data Engineering Patterns

Lambda Architecture: Batch and real-time processing
Medallion Architecture: Bronze → Silver → Gold data progression
Idempotency: Ensuring pipeline can be safely re-run
Data Quality: Validation, monitoring, alerting